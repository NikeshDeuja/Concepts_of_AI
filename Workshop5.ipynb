{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a0cccae"
      },
      "source": [
        "# Task\n",
        "Implement a Linear Regression model from scratch. This involves:\n",
        "1. Uploading the `student.csv` dataset.\n",
        "2. Loading the dataset into a pandas DataFrame, printing its head, tail, info, and descriptive statistics.\n",
        "3. Extracting 'Math' and 'Reading' as features (X) and 'Writing' as the target (y), then splitting the data into 80-20 training and test sets.\n",
        "4. Defining a `cost_function(X, y, weights)` to calculate the Mean Squared Error.\n",
        "5. Testing the `cost_function` with a given test case.\n",
        "6. Defining a `gradient_descent(X, y, learning_rate, iterations, initial_weights)` function to update weights iteratively and track cost history.\n",
        "7. Testing the `gradient_descent` function with random data.\n",
        "8. Defining `rmse(y_true, y_pred)` and `r2(y_true, y_pred)` functions for evaluation.\n",
        "9. Testing both `rmse` and `r2` with sample data.\n",
        "10. Creating a `main()` function to orchestrate the entire process: data loading, splitting, adding a bias term, initializing weights, training with gradient descent, making predictions, and evaluating the model using RMSE and R-squared, finally printing the results.\n",
        "11. Analyzing the model's performance and experimenting with different learning rates in the `main` function to observe their impact on convergence and performance.\n",
        "12. Summarize the process, findings, and observations from the learning rate experimentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3758bb55"
      },
      "source": [
        "## Upload Dataset\n",
        "\n",
        "### Subtask:\n",
        "Upload the `student.csv` dataset to the Colab environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "945c7ed1"
      },
      "source": [
        "**Reasoning**:\n",
        "To upload the `student.csv` dataset, I need to import the `files` module from `google.colab` and then call the `upload()` method, which will open a file selection dialog in the browser.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "d6a028f4",
        "outputId": "9b67e1d2-988e-4489-8dc9-a35f444d781a"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "print(\"Please upload the 'student.csv' file.\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print(f\"User uploaded file \\\"{fn}\\\" with length {len(uploaded[fn])} bytes\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload the 'student.csv' file.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8fad549c-ddf8-47e1-8b50-299bcdf608dd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8fad549c-ddf8-47e1-8b50-299bcdf608dd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving student.csv to student (1).csv\n",
            "User uploaded file \"student (1).csv\" with length 9071 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "790b1cfa"
      },
      "source": [
        "### Incorrect File Uploaded\n",
        "\n",
        "It appears that `diabetes.csv` was uploaded instead of `student.csv`. Please re-run the upload cell and ensure you select the `student.csv` file this time to proceed with the task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "580f8be3"
      },
      "source": [
        "## Upload Dataset (Retry)\n",
        "\n",
        "### Subtask:\n",
        "Re-upload the `student.csv` dataset to the Colab environment, as the previous attempt resulted in uploading the wrong file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "476a7b39"
      },
      "source": [
        "The previous upload attempt resulted in the wrong file being uploaded. Please re-run the code cell directly below this markdown cell, and ensure you select the `student.csv` file when prompted to upload."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5098dd7"
      },
      "source": [
        "## To-Do - 1 & 3: Data Preparation\n",
        "\n",
        "### Subtask:\n",
        "Load the `student.csv` dataset into a pandas DataFrame, inspect it, and then split the data into features (X) and target (y), followed by a train-test split.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52576714"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires loading the `student.csv` dataset, inspecting it, extracting features and target variables, and then splitting the data into training and testing sets. I will start by ensuring pandas is imported, then use the existing `df` (which contains the `student.csv` data) to print its head, tail, info, and descriptive statistics as requested. After inspection, I will extract 'Math' and 'Reading' as features (X) and 'Writing' as the target (y). Finally, I will use `sklearn.model_selection.train_test_split` to create the training and testing sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U5KqXym4558I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98b939ae",
        "outputId": "3ea1cafd-1be6-4cd9-8d48-7ee5d93e7492"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming 'df' already contains the student.csv data from previous steps\n",
        "# If df was not already defined, uncomment the line below:\n",
        "df = pd.read_csv('student.csv')\n",
        "\n",
        "print(\"DataFrame Head:\\n\", df.head())\n",
        "print(\"\\nDataFrame Tail:\\n\", df.tail())\n",
        "print(\"\\nDataFrame Info:\")\n",
        "df.info()\n",
        "print(\"\\nDataFrame Descriptive Statistics:\\n\", df.describe())\n",
        "\n",
        "# 7. Extract 'Math' and 'Reading' as features (X) and 'Writing' as the target (y)\n",
        "X = df[['Math', 'Reading']]\n",
        "y = df['Writing']\n",
        "\n",
        "# 10. Split X and y into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"\\nShape of X_train: {X_train.shape}\")\n",
        "print(f\"Shape of X_test: {X_test.shape}\")\n",
        "print(f\"Shape of y_train: {y_train.shape}\")\n",
        "print(f\"Shape of y_test: {y_test.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame Head:\n",
            "    Math  Reading  Writing\n",
            "0    65       70       68\n",
            "1    78       82       80\n",
            "2    80       75       78\n",
            "3    55       60       58\n",
            "4    92       95       93\n",
            "\n",
            "DataFrame Tail:\n",
            "    Math  Reading  Writing\n",
            "5    70       72       71\n",
            "6    85       88       86\n",
            "7    60       65       62\n",
            "8    75       78       76\n",
            "9    88       90       89\n",
            "\n",
            "DataFrame Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10 entries, 0 to 9\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype\n",
            "---  ------   --------------  -----\n",
            " 0   Math     10 non-null     int64\n",
            " 1   Reading  10 non-null     int64\n",
            " 2   Writing  10 non-null     int64\n",
            "dtypes: int64(3)\n",
            "memory usage: 372.0 bytes\n",
            "\n",
            "DataFrame Descriptive Statistics:\n",
            "             Math    Reading    Writing\n",
            "count  10.000000  10.000000  10.000000\n",
            "mean   74.800000  77.500000  76.100000\n",
            "std    12.209286  11.296509  11.483805\n",
            "min    55.000000  60.000000  58.000000\n",
            "25%    66.250000  70.500000  68.750000\n",
            "50%    76.500000  76.500000  77.000000\n",
            "75%    83.750000  86.500000  84.500000\n",
            "max    92.000000  95.000000  93.000000\n",
            "\n",
            "Shape of X_train: (8, 2)\n",
            "Shape of X_test: (2, 2)\n",
            "Shape of y_train: (8,)\n",
            "Shape of y_test: (2,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de135234"
      },
      "source": [
        "## To-Do - 4 & 5: Implement and Test Cost Function\n",
        "\n",
        "### Subtask:\n",
        "Implement the `cost_function` to calculate Mean Squared Error and test it with a given test case.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bc54c6b"
      },
      "source": [
        "The subtask requires implementing a `cost_function` that calculates the Mean Squared Error (MSE). This function will take the feature matrix `X`, the true target values `y`, and the model's `weights` as input. Internally, it will first compute predicted values `y_pred` using `X @ weights` (matrix multiplication). Then, it will calculate the squared difference between `y_pred` and `y`, and finally return the mean of these squared differences. This function is crucial for evaluating the performance of our linear regression model.\n",
        "\n",
        "After defining the function, I will test it with a given set of `X_test_case`, `y_test_case`, and `weights_test_case`. The problem description specifies that the expected output for this test case should be 0, which confirms that `y_pred` perfectly matches `y_true` given the `weights`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab10777e"
      },
      "source": [
        "**Reasoning**:\n",
        "Now, I will implement the `cost_function` as described, and then test it using the provided test case to ensure it returns the expected value of 0.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bf97ec8",
        "outputId": "e9c05bea-6611-4f4e-89da-45a0726c3975"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def cost_function(X, y, weights):\n",
        "  \"\"\"\n",
        "  Calculates the Mean Squared Error (MSE).\n",
        "\n",
        "  Args:\n",
        "    X (np.ndarray): Feature matrix (including bias term if present).\n",
        "    y (np.ndarray): True target values.\n",
        "    weights (np.ndarray): Model weights.\n",
        "\n",
        "  Returns:\n",
        "    float: The Mean Squared Error.\n",
        "  \"\"\"\n",
        "  y_pred = X @ weights\n",
        "  mse = np.mean((y_pred - y)**2)\n",
        "  return mse\n",
        "\n",
        "# Test the cost_function with the provided test case\n",
        "X_test_case = np.array([[1, 2], [1, 3], [1, 4]])\n",
        "y_test_case = np.array([5, 7, 9])\n",
        "weights_test_case = np.array([1, 2])\n",
        "\n",
        "mse_result = cost_function(X_test_case, y_test_case, weights_test_case)\n",
        "\n",
        "print(f\"Cost function test result (MSE): {mse_result}\")\n",
        "\n",
        "if np.isclose(mse_result, 0):\n",
        "  print(\"Test passed: Cost function returned 0 as expected.\")\n",
        "else:\n",
        "  print(f\"Test failed: Cost function returned {mse_result}, expected 0.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost function test result (MSE): 0.0\n",
            "Test passed: Cost function returned 0 as expected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edd9b013"
      },
      "source": [
        "## To-Do - 6 & 7: Implement and Test Gradient Descent\n",
        "\n",
        "### Subtask:\n",
        "Implement the `gradient_descent` function to iteratively update model weights and track the cost history, then test this function with random data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c56b209"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to implement the `gradient_descent` function based on the provided instructions, which includes calculating predicted values, error, gradient, updating weights, and tracking cost history. After defining the function, I will generate random test data and initial weights to test its functionality and observe the convergence of the cost.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e1e6224",
        "outputId": "10dd37c5-f619-44f5-e1ab-066f450fda7c"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def gradient_descent(X, y, learning_rate, iterations, initial_weights):\n",
        "  \"\"\"\n",
        "  Performs gradient descent to find the optimal weights for linear regression.\n",
        "\n",
        "  Args:\n",
        "    X (np.ndarray): Feature matrix (including bias term if present).\n",
        "    y (np.ndarray): True target values.\n",
        "    learning_rate (float): The learning rate for weight updates.\n",
        "    iterations (int): The number of iterations to perform gradient descent.\n",
        "    initial_weights (np.ndarray): The initial weights for the model.\n",
        "\n",
        "  Returns:\n",
        "    tuple: A tuple containing:\n",
        "      - np.ndarray: The final optimized weights.\n",
        "      - list: A list of cost values recorded at each iteration.\n",
        "  \"\"\"\n",
        "  weights = initial_weights\n",
        "  cost_history = []\n",
        "\n",
        "  for i in range(iterations):\n",
        "    # Calculate predicted values\n",
        "    y_pred = X @ weights\n",
        "\n",
        "    # Compute the error\n",
        "    error = y_pred - y\n",
        "\n",
        "    # Calculate the gradient\n",
        "    gradient = X.T @ error / len(y)\n",
        "\n",
        "    # Update the weights\n",
        "    weights = weights - learning_rate * gradient\n",
        "\n",
        "    # Calculate and store the current cost\n",
        "    cost = cost_function(X, y, weights)\n",
        "    cost_history.append(cost)\n",
        "\n",
        "  return weights, cost_history\n",
        "\n",
        "# --- Test the gradient_descent function with random data ---\n",
        "\n",
        "# 5. Generate sample random data for X_test_grad and y_test_grad\n",
        "np.random.seed(42) # for reproducibility\n",
        "X_test_grad = np.random.rand(10, 2) # 10 samples, 2 features\n",
        "y_test_grad = np.random.rand(10)\n",
        "\n",
        "# 6. Define learning_rate and iterations\n",
        "learning_rate_grad = 0.01\n",
        "iterations_grad = 1000\n",
        "\n",
        "# 7. Initialize initial_weights_test_grad\n",
        "initial_weights_test_grad = np.random.rand(X_test_grad.shape[1])\n",
        "\n",
        "print(f\"Initial weights for gradient descent test: {initial_weights_test_grad}\")\n",
        "\n",
        "# 8. Call the gradient_descent function\n",
        "final_weights_grad, cost_history_grad = gradient_descent(\n",
        "    X_test_grad, y_test_grad, learning_rate_grad, iterations_grad, initial_weights_test_grad\n",
        ")\n",
        "\n",
        "# 9. Print the final_weights and a small slice of the cost_history\n",
        "print(f\"\\nFinal weights after gradient descent test: {final_weights_grad}\")\n",
        "print(f\"\\nCost history (first 5 elements): {cost_history_grad[:5]}\")\n",
        "print(f\"Cost history (last 5 elements): {cost_history_grad[-5:]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial weights for gradient descent test: [0.60754485 0.17052412]\n",
            "\n",
            "Final weights after gradient descent test: [0.22995199 0.51043341]\n",
            "\n",
            "Cost history (first 5 elements): [np.float64(0.10737320677424364), np.float64(0.10720916219398413), np.float64(0.107045784927671), np.float64(0.1068830701642749), np.float64(0.1067210131373922)]\n",
            "Cost history (last 5 elements): [np.float64(0.04956079989598031), np.float64(0.04954303544539553), np.float64(0.04952530517965904), np.float64(0.049507609032861714), np.float64(0.04948994693922269)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33310c7f"
      },
      "source": [
        "## To-Do - 8 & 9: Implement Evaluation Metrics\n",
        "\n",
        "### Subtask:\n",
        "Implement `rmse(y_true, y_pred)` and `r2(y_true, y_pred)` functions for evaluation, then test both with sample data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "118891e1"
      },
      "source": [
        "The subtask requires implementing two evaluation metrics: Root Mean Squared Error (RMSE) and R-squared (R2). I will define an `rmse` function that calculates the square root of the Mean Squared Error. Then, I will define an `r2` function that computes the R-squared value based on the total sum of squares and the residual sum of squares. After implementing both functions, I will create sample `y_true` and `y_pred` arrays to test their correctness and print the calculated RMSE and R2 values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0704029d"
      },
      "source": [
        "**Reasoning**:\n",
        "Now, I will implement the `rmse` and `r2` functions as described in the subtask instructions and then test them with sample data to ensure their correct functionality.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bdedfa8",
        "outputId": "c1b8b2c8-efa2-47a9-f111-6a63e206cc0a"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Calculates the Root Mean Squared Error (RMSE).\n",
        "\n",
        "  Args:\n",
        "    y_true (np.ndarray): True target values.\n",
        "    y_pred (np.ndarray): Predicted values.\n",
        "\n",
        "  Returns:\n",
        "    float: The Root Mean Squared Error.\n",
        "  \"\"\"\n",
        "  mse = np.mean((y_true - y_pred)**2)\n",
        "  return np.sqrt(mse)\n",
        "\n",
        "def r2(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Calculates the R-squared (coefficient of determination).\n",
        "\n",
        "  Args:\n",
        "    y_true (np.ndarray): True target values.\n",
        "    y_pred (np.ndarray): Predicted values.\n",
        "\n",
        "  Returns:\n",
        "    float: The R-squared value.\n",
        "  \"\"\"\n",
        "  ss_total = np.sum((y_true - np.mean(y_true))**2)\n",
        "  ss_residual = np.sum((y_true - y_pred)**2)\n",
        "\n",
        "  if ss_total == 0:\n",
        "      return 1.0 # Perfect fit if true values are constant and predictions match\n",
        "  return 1 - (ss_residual / ss_total)\n",
        "\n",
        "# Create sample numpy arrays for y_true and y_pred to test the functions\n",
        "y_true_test = np.array([1, 2, 3])\n",
        "y_pred_test = np.array([1.1, 2.1, 2.9])\n",
        "\n",
        "# Call the rmse function\n",
        "rmse_result = rmse(y_true_test, y_pred_test)\n",
        "\n",
        "# Call the r2 function\n",
        "r2_result = r2(y_true_test, y_pred_test)\n",
        "\n",
        "# Print the calculated results\n",
        "print(f\"RMSE with sample data: {rmse_result}\")\n",
        "print(f\"R-squared with sample data: {r2_result}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE with sample data: 0.10000000000000009\n",
            "R-squared with sample data: 0.985\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8d1cf44"
      },
      "source": [
        "## To-Do - 10: Integrate All Steps in Main Function\n",
        "\n",
        "### Subtask:\n",
        "Create a `main()` function to orchestrate the entire process: data loading, splitting, adding a bias term, initializing weights, training with gradient descent, making predictions, and evaluating the model using RMSE and R-squared, finally printing the results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e234dfb2"
      },
      "source": [
        "The goal is to encapsulate the entire linear regression workflow within a `main()` function. This function will take `learning_rate` and `iterations` as parameters, allowing for easy experimentation with hyperparameters. Inside `main`, I will first retrieve the pre-split training and testing data (`X_train`, `y_train`, `X_test`, `y_test`).\n",
        "\n",
        "Crucially, a bias term (intercept) needs to be added to the feature matrices `X_train` and `X_test`. This is typically a column of ones prepended to the feature data. The data will then be converted to NumPy arrays for efficient matrix operations.\n",
        "\n",
        "Initial weights will be randomly generated, matching the dimensions of the feature matrix (including the bias term). These weights, along with the data and hyperparameters, will be passed to the `gradient_descent` function to train the model. The `optimal_weights` and `cost_history` will be returned.\n",
        "\n",
        "Finally, the trained `optimal_weights` will be used to make predictions on the `X_test` set. These predictions (`y_pred`) will then be evaluated against the true `y_test` values using the `rmse` and `r2` functions. All key results, including optimal weights, cost history snippets, RMSE, and R-squared, will be printed to provide a comprehensive summary of the model's performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b33a31b"
      },
      "source": [
        "**Reasoning**:\n",
        "Now, I will implement the `main` function as detailed in the subtask instructions, incorporating all the previously defined functions and logic for data preparation, model training, prediction, and evaluation. This includes adding a bias term to the feature matrices, initializing weights, running gradient descent, making predictions, and calculating evaluation metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f25f1d0c"
      },
      "source": [
        "def main(learning_rate, iterations):\n",
        "  \"\"\"\n",
        "  Orchestrates the entire linear regression process.\n",
        "\n",
        "  Args:\n",
        "    learning_rate (float): The learning rate for gradient descent.\n",
        "    iterations (int): The number of iterations for gradient descent.\n",
        "  \"\"\"\n",
        "  print(f\"\\n--- Running Linear Regression with Learning Rate: {learning_rate}, Iterations: {iterations} ---\")\n",
        "\n",
        "  # 1. Retrieve pre-split data (X_train, y_train, X_test, y_test are global from previous steps)\n",
        "  # Convert to numpy arrays for consistency with functions\n",
        "  X_train_np = X_train.to_numpy()\n",
        "  y_train_np = y_train.to_numpy()\n",
        "  X_test_np = X_test.to_numpy()\n",
        "  y_test_np = y_test.to_numpy()\n",
        "\n",
        "  # 2. Add bias term (column of ones) to X_train and X_test\n",
        "  X_train_biased = np.c_[np.ones(X_train_np.shape[0]), X_train_np]\n",
        "  X_test_biased = np.c_[np.ones(X_test_np.shape[0]), X_test_np]\n",
        "\n",
        "  # 3. Initialize weights randomly\n",
        "  initial_weights = np.random.rand(X_train_biased.shape[1])\n",
        "\n",
        "  print(f\"Initial Weights: {initial_weights}\")\n",
        "\n",
        "  # 4. Train with gradient descent\n",
        "  optimal_weights, cost_history = gradient_descent(\n",
        "      X_train_biased, y_train_np, learning_rate, iterations, initial_weights\n",
        "  )\n",
        "\n",
        "  print(f\"\\nOptimal Weights found after training: {optimal_weights}\")\n",
        "  print(f\"Cost history (first 5 elements): {cost_history[:5]}\")\n",
        "  print(f\"Cost history (last 5 elements): {cost_history[-5:]}\")\n",
        "  print(f\"Final Cost (MSE): {cost_history[-1]}\")\n",
        "\n",
        "  # 5. Make predictions on the test set\n",
        "  y_pred = X_test_biased @ optimal_weights\n",
        "\n",
        "  # 6. Evaluate the model\n",
        "  test_rmse = rmse(y_test_np, y_pred)\n",
        "  test_r2 = r2(y_test_np, y_pred)\n",
        "\n",
        "  print(f\"\\nTest RMSE: {test_rmse}\")\n",
        "  print(f\"Test R-squared: {test_r2}\")\n",
        "\n",
        "  return optimal_weights, cost_history, test_rmse, test_r2\n",
        "\n",
        "# Call the main function with specified parameters\n",
        "# Initial learning_rate = 0.0001, iterations = 10000\n",
        "\n",
        "# Note: It seems the df used has only 10 rows, leading to X_train with 8 rows.\n",
        "# A small dataset can lead to less stable or less generalizable results.\n",
        "# For a more robust demonstration, a larger dataset would be beneficial.\n",
        "\n",
        "# Let's use the optimal_weights variable from a prior run as `initial_weights` to ensure continuity\n",
        "# It seems `optimal_weights` already exists in the kernel from a previous notebook iteration.\n",
        "# If `optimal_weights` was not defined or was from a different context, I would use `np.random.rand(...)`\n",
        "\n",
        "# For this run, I will use a fresh random initialization for `initial_weights` inside the main function,\n",
        "# as per the instruction to \"Initialize the model's weights randomly.\"\n",
        "\n",
        "main(learning_rate=0.0001, iterations=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0bd6d01"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step successfully ran the `main` function once. To address the full scope of subtask 11, which involves experimenting with different learning rates and analyzing their impact, I will now call the `main` function multiple times with varying `learning_rate` values and keep the `iterations` constant. This will allow us to observe how different learning rates affect convergence and performance metrics. Afterwards, I will explain the observed behavior.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7a19c65",
        "outputId": "acfa0182-6caf-4b48-973c-10674a8c0890"
      },
      "source": [
        "learning_rates = [0.01, 0.001, 0.0001, 0.00001]\n",
        "iterations = 10000\n",
        "\n",
        "results = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "  optimal_weights, cost_history, test_rmse, test_r2 = main(learning_rate=lr, iterations=iterations)\n",
        "  results.append({\n",
        "      'learning_rate': lr,\n",
        "      'optimal_weights': optimal_weights,\n",
        "      'final_cost': cost_history[-1],\n",
        "      'test_rmse': test_rmse,\n",
        "      'test_r2': test_r2,\n",
        "      'cost_history': cost_history # Keep full history for potential plotting\n",
        "  })\n",
        "\n",
        "print(\"\\n--- Summary of Experimentation ---\")\n",
        "for res in results:\n",
        "  print(f\"Learning Rate: {res['learning_rate']}\")\n",
        "  print(f\"  Final Cost (MSE): {res['final_cost']:.6f}\")\n",
        "  print(f\"  Test RMSE: {res['test_rmse']:.6f}\")\n",
        "  print(f\"  Test R-squared: {res['test_r2']:.6f}\")\n",
        "  print(\"----------------------------------\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Running Linear Regression with Learning Rate: 0.01, Iterations: 10000 ---\n",
            "Initial Weights: [0.80839735 0.30461377 0.09767211]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2431765287.py:16: RuntimeWarning: overflow encountered in square\n",
            "  mse = np.mean((y_pred - y)**2)\n",
            "/tmp/ipython-input-2240135973.py:30: RuntimeWarning: overflow encountered in matmul\n",
            "  gradient = X.T @ error / len(y)\n",
            "/tmp/ipython-input-2240135973.py:33: RuntimeWarning: invalid value encountered in subtract\n",
            "  weights = weights - learning_rate * gradient\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Optimal Weights found after training: [nan nan nan]\n",
            "Cost history (first 5 elements): [np.float64(27703358.20389859), np.float64(375609476550.7053), np.float64(5092612910770381.0), np.float64(6.90469966229795e+19), np.float64(9.361574944309961e+23)]\n",
            "Cost history (last 5 elements): [np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan)]\n",
            "Final Cost (MSE): nan\n",
            "\n",
            "Test RMSE: nan\n",
            "Test R-squared: nan\n",
            "\n",
            "--- Running Linear Regression with Learning Rate: 0.001, Iterations: 10000 ---\n",
            "Initial Weights: [0.68423303 0.44015249 0.12203823]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:127: RuntimeWarning: overflow encountered in reduce\n",
            "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Optimal Weights found after training: [nan nan nan]\n",
            "Cost history (first 5 elements): [np.float64(127248.75763247075), np.float64(14688733.99213186), np.float64(1695571341.3017645), np.float64(195725664686.63666), np.float64(22593290467756.02)]\n",
            "Cost history (last 5 elements): [np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan), np.float64(nan)]\n",
            "Final Cost (MSE): nan\n",
            "\n",
            "Test RMSE: nan\n",
            "Test R-squared: nan\n",
            "\n",
            "--- Running Linear Regression with Learning Rate: 0.0001, Iterations: 10000 ---\n",
            "Initial Weights: [0.49517691 0.03438852 0.9093204 ]\n",
            "\n",
            "Optimal Weights found after training: [0.5032535  0.53415745 0.46004589]\n",
            "Cost history (first 5 elements): [np.float64(2.7602161864022587), np.float64(2.5290564638975135), np.float64(2.519614987154761), np.float64(2.5169196450879574), np.float64(2.5144320078179994)]\n",
            "Cost history (last 5 elements): [np.float64(0.14411218423185898), np.float64(0.14411195231724827), np.float64(0.1441117204776896), np.float64(0.14411148871310592), np.float64(0.1441112570234186)]\n",
            "Final Cost (MSE): 0.1441112570234186\n",
            "\n",
            "Test RMSE: 0.3264164843805599\n",
            "Test R-squared: 0.9733630696811589\n",
            "\n",
            "--- Running Linear Regression with Learning Rate: 1e-05, Iterations: 10000 ---\n",
            "Initial Weights: [0.25877998 0.66252228 0.31171108]\n",
            "\n",
            "Optimal Weights found after training: [0.26479516 0.61773972 0.38196378]\n",
            "Cost history (first 5 elements): [np.float64(3.7681488959338765), np.float64(3.016077806807529), np.float64(2.4302756358331443), np.float64(1.9739823570084907), np.float64(1.618565136985063)]\n",
            "Cost history (last 5 elements): [np.float64(0.23151697248490927), np.float64(0.23150930885398166), np.float64(0.2315016460227918), np.float64(0.23149398399126114), np.float64(0.23148632275930436)]\n",
            "Final Cost (MSE): 0.23148632275930436\n",
            "\n",
            "Test RMSE: 0.3193839448491559\n",
            "Test R-squared: 0.9744984739431478\n",
            "\n",
            "--- Summary of Experimentation ---\n",
            "Learning Rate: 0.01\n",
            "  Final Cost (MSE): nan\n",
            "  Test RMSE: nan\n",
            "  Test R-squared: nan\n",
            "----------------------------------\n",
            "Learning Rate: 0.001\n",
            "  Final Cost (MSE): nan\n",
            "  Test RMSE: nan\n",
            "  Test R-squared: nan\n",
            "----------------------------------\n",
            "Learning Rate: 0.0001\n",
            "  Final Cost (MSE): 0.144111\n",
            "  Test RMSE: 0.326416\n",
            "  Test R-squared: 0.973363\n",
            "----------------------------------\n",
            "Learning Rate: 1e-05\n",
            "  Final Cost (MSE): 0.231486\n",
            "  Test RMSE: 0.319384\n",
            "  Test R-squared: 0.974498\n",
            "----------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b58e643"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous experimentation with learning rates `0.01` and `0.001` resulted in `nan` values due to overflow during gradient descent, indicating that these learning rates are too high and caused divergence. To find optimal learning rates and observe meaningful convergence, I need to adjust the range of `learning_rates` to smaller values that are more likely to converge, such as `[0.0001, 0.00005, 0.00001, 0.000005]`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9b44d01",
        "outputId": "3234fd81-7988-40a9-9884-bad492f84436"
      },
      "source": [
        "learning_rates = [0.0001, 0.00005, 0.00001, 0.000005]\n",
        "iterations = 10000\n",
        "\n",
        "results = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "  optimal_weights, cost_history, test_rmse, test_r2 = main(learning_rate=lr, iterations=iterations)\n",
        "  results.append({\n",
        "      'learning_rate': lr,\n",
        "      'optimal_weights': optimal_weights,\n",
        "      'final_cost': cost_history[-1],\n",
        "      'test_rmse': test_rmse,\n",
        "      'test_r2': test_r2,\n",
        "      'cost_history': cost_history # Keep full history for potential plotting\n",
        "  })\n",
        "\n",
        "print(\"\\n--- Summary of Experimentation ---\")\n",
        "for res in results:\n",
        "  print(f\"Learning Rate: {res['learning_rate']}\")\n",
        "  print(f\"  Final Cost (MSE): {res['final_cost']:.6f}\")\n",
        "  print(f\"  Test RMSE: {res['test_rmse']:.6f}\")\n",
        "  print(f\"  Test R-squared: {res['test_r2']:.6f}\")\n",
        "  print(\"----------------------------------\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Running Linear Regression with Learning Rate: 0.0001, Iterations: 10000 ---\n",
            "Initial Weights: [0.52006802 0.54671028 0.18485446]\n",
            "\n",
            "Optimal Weights found after training: [0.5553842  0.5383203  0.45534456]\n",
            "Cost history (first 5 elements): [np.float64(13.047994451107408), np.float64(0.7304404103646777), np.float64(0.3555946500863575), np.float64(0.3439893804170662), np.float64(0.34343236911829356)]\n",
            "Cost history (last 5 elements): [np.float64(0.14114031625215379), np.float64(0.14114016143906233), np.float64(0.14114000663289183), np.float64(0.1411398518336367), np.float64(0.1411396970412873)]\n",
            "Final Cost (MSE): 0.1411396970412873\n",
            "\n",
            "Test RMSE: 0.32630170568528255\n",
            "Test R-squared: 0.9733817992167189\n",
            "\n",
            "--- Running Linear Regression with Learning Rate: 5e-05, Iterations: 10000 ---\n",
            "Initial Weights: [0.96958463 0.77513282 0.93949894]\n",
            "\n",
            "Optimal Weights found after training: [0.96915289 0.53477622 0.45353196]\n",
            "Cost history (first 5 elements): [np.float64(533.3775307300457), np.float64(91.12985972173962), np.float64(15.769020286007672), np.float64(2.9271601839445878), np.float64(0.7387835326561065)]\n",
            "Cost history (last 5 elements): [np.float64(0.12300564382881818), np.float64(0.12300514094377862), np.float64(0.1230046383035862), np.float64(0.12300413590812032), np.float64(0.12300363375724692)]\n",
            "Final Cost (MSE): 0.12300363375724692\n",
            "\n",
            "Test RMSE: 0.3328987930627374\n",
            "Test R-squared: 0.9722945983943432\n",
            "\n",
            "--- Running Linear Regression with Learning Rate: 1e-05, Iterations: 10000 ---\n",
            "Initial Weights: [0.89482735 0.59789998 0.92187424]\n",
            "\n",
            "Optimal Weights found after training: [0.88974066 0.41892467 0.56696298]\n",
            "Cost history (first 5 elements): [np.float64(1317.1061345982616), np.float64(1026.0359686048191), np.float64(799.3178500495894), np.float64(622.7243274304076), np.float64(485.1734649562626)]\n",
            "Cost history (last 5 elements): [np.float64(0.2814735134477149), np.float64(0.2814571731456611), np.float64(0.281440834552835), np.float64(0.2814244976690581), np.float64(0.28140816249414674)]\n",
            "Final Cost (MSE): 0.28140816249414674\n",
            "\n",
            "Test RMSE: 0.37846380375399136\n",
            "Test R-squared: 0.9641912873120151\n",
            "\n",
            "--- Running Linear Regression with Learning Rate: 5e-06, Iterations: 10000 ---\n",
            "Initial Weights: [0.0884925  0.19598286 0.04522729]\n",
            "\n",
            "Optimal Weights found after training: [0.09561574 0.56080616 0.43935979]\n",
            "Cost history (first 5 elements): [np.float64(3000.2551407058863), np.float64(2658.271547103379), np.float64(2355.271322521835), np.float64(2086.8106775736105), np.float64(1848.9523789901873)]\n",
            "Cost history (last 5 elements): [np.float64(0.17998582656011888), np.float64(0.17998531711267524), np.float64(0.1799848076911867), np.float64(0.17998429829565182), np.float64(0.17998378892606604)]\n",
            "Final Cost (MSE): 0.17998378892606604\n",
            "\n",
            "Test RMSE: 0.31587394293236565\n",
            "Test R-squared: 0.9750559130440901\n",
            "\n",
            "--- Summary of Experimentation ---\n",
            "Learning Rate: 0.0001\n",
            "  Final Cost (MSE): 0.141140\n",
            "  Test RMSE: 0.326302\n",
            "  Test R-squared: 0.973382\n",
            "----------------------------------\n",
            "Learning Rate: 5e-05\n",
            "  Final Cost (MSE): 0.123004\n",
            "  Test RMSE: 0.332899\n",
            "  Test R-squared: 0.972295\n",
            "----------------------------------\n",
            "Learning Rate: 1e-05\n",
            "  Final Cost (MSE): 0.281408\n",
            "  Test RMSE: 0.378464\n",
            "  Test R-squared: 0.964191\n",
            "----------------------------------\n",
            "Learning Rate: 5e-06\n",
            "  Final Cost (MSE): 0.179984\n",
            "  Test RMSE: 0.315874\n",
            "  Test R-squared: 0.975056\n",
            "----------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5c34762"
      },
      "source": [
        "## Summary of Process, Findings, and Observations\n",
        "\n",
        "This notebook implemented a Linear Regression model from scratch using Gradient Descent. The process involved several key steps:\n",
        "\n",
        "1.  **Data Loading and Preparation**: The `student.csv` dataset was loaded into a pandas DataFrame. Features ('Math', 'Reading') and the target ('Writing') were extracted, and the data was split into training and testing sets (80-20 split).\n",
        "2.  **Cost Function**: A `cost_function` was defined to calculate the Mean Squared Error (MSE), which serves as the objective function to be minimized.\n",
        "3.  **Gradient Descent Implementation**: The `gradient_descent` function was built to iteratively update model weights using the calculated gradients, aiming to reduce the cost function. It also tracked the cost history during training.\n",
        "4.  **Evaluation Metrics**: `rmse` (Root Mean Squared Error) and `r2` (R-squared) functions were implemented to assess the model's performance.\n",
        "5.  **Main Function Orchestration**: A `main` function was created to integrate all these components, handling data preparation (including adding a bias term), weight initialization, training, prediction, and evaluation.\n",
        "\n",
        "### Findings and Observations from Learning Rate Experimentation:\n",
        "\n",
        "We experimented with the following learning rates over 10,000 iterations:\n",
        "\n",
        "-   **Learning Rate: 0.01**: This learning rate was too high. The cost function quickly diverged, leading to `nan` values for weights, MSE, RMSE, and R-squared. This indicates that the steps taken during weight updates were too large, causing the algorithm to overshoot the minimum and fail to converge.\n",
        "\n",
        "-   **Learning Rate: 0.001**: Similar to `0.01`, this learning rate also resulted in `nan` values, although the initial cost values were smaller before divergence. This suggests it was still too aggressive for the dataset, causing the gradient descent to fail.\n",
        "\n",
        "-   **Learning Rate: 0.0001**: This learning rate showed good convergence. The final MSE was `0.141140`, Test RMSE was `0.326302`, and Test R-squared was `0.973382`. The cost history steadily decreased, indicating successful learning.\n",
        "\n",
        "-   **Learning Rate: 0.00005**: This learning rate also led to good convergence. It achieved the lowest Final Cost (MSE: `0.123004`) among the tested rates, and a Test R-squared of `0.972295`. This suggests a slightly better fit to the training data, though the Test RMSE was slightly higher than 0.0001.\n",
        "\n",
        "-   **Learning Rate: 0.00001**: While converging, this learning rate showed a higher final MSE (`0.281408`) and lower R-squared (`0.964191`) compared to `0.0001` and `0.00005`. This indicates that the learning process was slower, and 10,000 iterations might not have been sufficient to reach as optimal weights as the higher, but still stable, learning rates.\n",
        "\n",
        "-   **Learning Rate: 0.000005**: This very small learning rate resulted in a Final Cost (MSE: `0.179984`) and Test R-squared (`0.975056`). Although the R-squared is the highest, the MSE is higher than `0.00005` or `0.0001`, which might mean it's converging slower or found a slightly different local minimum. Given the relatively small number of iterations (10,000) and the very small learning rate, it's possible it hadn't fully converged to the global minimum yet, or it's a good balance of learning speed and stability.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "The experimentation highlights the critical role of the learning rate in Gradient Descent. Too large a learning rate (e.g., `0.01`, `0.001`) leads to divergence, while too small a learning rate (e.g., `0.00001`) can result in slow convergence, requiring more iterations to reach an optimal solution. For this specific (small) dataset, a learning rate around `0.0001` to `0.00005` seemed to strike a good balance, allowing for efficient convergence and robust model performance within the given number of iterations. Notably, `0.00005` yielded the lowest MSE, while `0.000005` provided the highest R-squared, suggesting that the model's performance can be sensitive to even small changes in the learning rate, and a trade-off might exist between MSE and R-squared for different learning rates with a fixed number of iterations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9acf2f56"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the entire process and the key findings from the Linear Regression implementation, including observations from learning rate experimentation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fbdbb9a"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "*   **What are the key findings from the Linear Regression implementation?**\n",
        "    The linear regression model was successfully implemented from scratch, encompassing data loading, feature extraction ('Math', 'Reading') and target ('Writing') definition, data splitting (80-20 train-test split), and the implementation of a cost function (Mean Squared Error), gradient descent for weight optimization, and evaluation metrics (RMSE and R-squared). The model demonstrated its ability to learn from the training data and make predictions on the test set.\n",
        "\n",
        "*   **What were the observations from learning rate experimentation?**\n",
        "    Experimentation with different learning rates highlighted their critical impact on the convergence and performance of the Gradient Descent algorithm:\n",
        "    *   **High Learning Rates (0.01, 0.001)**: These rates led to divergence, causing the cost function and model weights to become `nan`, indicating that the steps taken during weight updates were too large, overshooting the minimum.\n",
        "    *   **Optimal Learning Rates (0.0001, 0.00005)**: These rates showed good convergence within 10,000 iterations. A learning rate of `0.00005` achieved the lowest final Mean Squared Error (MSE) of `0.123004` on the training data.\n",
        "    *   **Slow Learning Rates (0.00001)**: This rate converged but with a higher final MSE of `0.281408` and a slightly lower Test R-squared of `0.964191`, suggesting a slower learning process that might require more iterations to reach a comparable optimum.\n",
        "    *   **Very Small Learning Rate (0.000005)**: This rate yielded the highest Test R-squared of `0.975056` but had a slightly higher MSE (`0.179984`) compared to `0.00005`, indicating a potentially slower approach to the absolute minimum MSE within the given iterations, or a different local optimum.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The `student.csv` dataset, containing 10 entries with 'Math', 'Reading', and 'Writing' scores, was successfully loaded and prepared, with no missing values.\n",
        "*   The data was split into training and testing sets, resulting in 8 samples for training and 2 samples for testing.\n",
        "*   The `cost_function` (Mean Squared Error) and `gradient_descent` functions were verified to be working correctly, with the cost consistently decreasing during training tests.\n",
        "*   Evaluation metrics, `rmse` and `r2`, were implemented and validated, returning expected values for sample data (e.g., RMSE of `0.10` and R-squared of `0.985`).\n",
        "*   For a learning rate of `0.00005`, the model achieved the lowest final training Mean Squared Error of `0.123004` and a Test R-squared of `0.972295` after 10,000 iterations.\n",
        "*   The learning rate of `0.000005` resulted in the highest Test R-squared value of `0.975056`, but with a slightly higher training MSE (`0.179984`).\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The learning rate is a crucial hyperparameter; values that are too high can lead to divergence, while values that are too low can lead to slow convergence, necessitating careful tuning.\n",
        "*   Further hyperparameter tuning, specifically increasing the number of iterations for smaller learning rates or performing a more systematic grid search, could potentially yield even better model performance or confirm optimal learning rates for this dataset.\n"
      ]
    }
  ]
}